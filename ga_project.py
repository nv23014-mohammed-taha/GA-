# -*- coding: utf-8 -*-
"""GA PROJECT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n_9K101jbZgPO49rpqspQQWV8V9CtTCt
"""
# -*- coding: utf-8 -*-
import streamlit as st
import pandas as pd
import numpy as np
import cv2
from datetime import datetime
from statistics import mean, mode
from tensorflow.keras.models import load_model
import matplotlib.pyplot as plt
import seaborn as sns
import os

# -------------------------------
# CONFIG
# -------------------------------
CSV_FILE = "weather_data.csv"
MODEL_PATH = "models/vgg19.h5"
IMG_SIZE = 150
CLASS_NAMES = ["cloudy", "foggy", "rainy", "shine", "sunrise"]

# Ensure CSV exists
if not os.path.exists(CSV_FILE):
    df_empty = pd.DataFrame(columns=["Date", "Temperature", "Condition", "Humidity", "WindSpeed"])
    df_empty.to_csv(CSV_FILE, index=False)

# Load trained model
model = load_model(MODEL_PATH)

# -------------------------------
# UTILS
# -------------------------------
def load_data():
    return pd.read_csv(CSV_FILE)

def save_data(df):
    df.to_csv(CSV_FILE, index=False)

def record_observation(date, temp, condition, humidity, wind):
    df = load_data()
    df = df.append({
        "Date": date,
        "Temperature": temp,
        "Condition": condition,
        "Humidity": humidity,
        "WindSpeed": wind
    }, ignore_index=True)
    save_data(df)

def get_statistics():
    df = load_data()
    avg_temp = df["Temperature"].mean()
    min_temp = df["Temperature"].min()
    max_temp = df["Temperature"].max()
    most_common_condition = df["Condition"].mode()[0] if not df.empty else "N/A"
    return avg_temp, min_temp, max_temp, most_common_condition

def search_by_date(date):
    df = load_data()
    return df[df["Date"] == date]

def predict_weather_from_image(image):
    # Read and preprocess image
    file_bytes = np.asarray(bytearray(image.read()), dtype=np.uint8)
    img = cv2.imdecode(file_bytes, 1)
    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))
    img = img / 255.0
    img = img.reshape(-1, IMG_SIZE, IMG_SIZE, 3)
    pred = model.predict(img)
    class_idx = np.argmax(pred)
    return CLASS_NAMES[class_idx], pred[0][class_idx]

def plot_temperature_trend(df):
    df["Date"] = pd.to_datetime(df["Date"], format="%m-%d-%Y")
    plt.figure(figsize=(12,5))
    sns.lineplot(x="Date", y="Temperature", data=df)
    plt.title("Temperature Trend")
    plt.xlabel("Date")
    plt.ylabel("Temperature (Â°C)")
    st.pyplot(plt.gcf())
    plt.clf()

# -------------------------------
# STREAMLIT APP
# -------------------------------
st.set_page_config(page_title="Weather Tracker", layout="wide")
st.title("ðŸŒ¤ Weather Tracker & Image Predictor")

menu = ["Record Observation", "View Stats", "Search by Date", "View All", "Predict from Image"]
choice = st.sidebar.selectbox("Menu", menu)

# -------------------------------
# RECORD OBSERVATION
# -------------------------------
if choice == "Record Observation":
    st.subheader("Record New Weather Observation")
    date = st.date_input("Date")
    temp = st.number_input("Temperature (Â°C)", min_value=-50, max_value=60)
    condition = st.selectbox("Condition", ["Sunny", "Cloudy", "Rainy", "Stormy", "Windy", "Foggy"])
    humidity = st.number_input("Humidity (%)", min_value=0, max_value=100)
    wind = st.number_input("Wind Speed (km/h)", min_value=0, max_value=150)
    
    if st.button("Record Observation"):
        record_observation(date.strftime("%m-%d-%Y"), temp, condition, humidity, wind)
        st.success("âœ… Observation Recorded")

# -------------------------------
# VIEW STATISTICS
# -------------------------------
elif choice == "View Stats":
    st.subheader("Weather Statistics")
    df = load_data()
    if df.empty:
        st.warning("No data available. Please record observations first.")
    else:
        avg_temp, min_temp, max_temp, common_condition = get_statistics()
        st.write(f"**Average Temperature:** {avg_temp:.1f}Â°C")
        st.write(f"**Min Temperature:** {min_temp}Â°C | **Max Temperature:** {max_temp}Â°C")
        st.write(f"**Most Common Condition:** {common_condition}")
        st.markdown("---")
        st.subheader("Temperature Trend")
        plot_temperature_trend(df)

# -------------------------------
# SEARCH BY DATE
# -------------------------------
elif choice == "Search by Date":
    st.subheader("Search Observations by Date")
    date = st.date_input("Enter Date")
    df_search = search_by_date(date.strftime("%m-%d-%Y"))
    if df_search.empty:
        st.info("No observations found for this date.")
    else:
        st.dataframe(df_search)

# -------------------------------
# VIEW ALL OBSERVATIONS
# -------------------------------
elif choice == "View All":
    st.subheader("All Recorded Weather Observations")
    df_all = load_data()
    if df_all.empty:
        st.warning("No data available.")
    else:
        st.dataframe(df_all)
        st.markdown("---")
        st.subheader("Temperature Trend")
        plot_temperature_trend(df_all)

# -------------------------------
# PREDICT FROM IMAGE
# -------------------------------
elif choice == "Predict from Image":
    st.subheader("Predict Weather from Image")
    uploaded_file = st.file_uploader("Upload a Weather Image", type=["jpg", "png", "jpeg"])
    if uploaded_file is not None:
        label, confidence = predict_weather_from_image(uploaded_file)
        st.image(uploaded_file, caption=f"Predicted: {label} ({confidence*100:.2f}%)", use_column_width=True)

import kagglehub
vijaygiitk_multiclass_weather_dataset_path = kagglehub.dataset_download('vijaygiitk/multiclass-weather-dataset')

print('Data source import complete.')
import os

path = kagglehub.dataset_download("vijaygiitk/multiclass-weather-dataset")

print("Path to dataset files:", path)

import csv
import os
from datetime import datetime
from statistics import mean, mode
import pandas as pd

import pandas as pd
import random
from datetime import datetime, timedelta


start_date = datetime(2025, 1, 1)
dates = [(start_date + timedelta(days=i)).strftime("%m-%d-%Y") for i in range(365)]


conditions = ["Sunny", "Cloudy", "Rainy", "Stormy", "Windy", "Foggy"]


data = {
    "Date": dates,
    "Temperature": [random.randint(15, 40) for _ in range(365)],      # Â°C
    "Condition": [random.choice(conditions) for _ in range(365)],
    "Humidity": [random.randint(40, 90) for _ in range(365)],         # %
    "WindSpeed": [random.randint(5, 25) for _ in range(365)]          # km/h
}

df = pd.DataFrame(data)


df.to_csv("weather_data.csv", index=False)

print("âœ… 1-Year (2025) Weather Dataset Created Successfully!")
print(f"Total Records: {len(df)}")
df.head(10)

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import random
import cv2
import tqdm as tqdm
from tensorflow.keras.preprocessing.image import load_img
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

root_dir = "/kaggle/input/multiclass-weather-dataset"
os.listdir(root_dir)
os.path.exists(root_dir)

# Cell A: Create a 1-year (2025) seasonally realistic weather dataset
import pandas as pd
import random
from datetime import datetime, timedelta

def generate_seasonal_temp(month):
    """Return a plausible temperature range for a month (Celsius)."""
    # Simple hemispheric model (adjust to your location if needed)
    # Jan (1) -> winter-like, Jul (7) -> summer-like (this is generic)
    month = int(month)
    if month in (12, 1, 2):   # winter
        return random.randint(8, 20)
    if month in (3, 4, 5):    # spring
        return random.randint(15, 26)
    if month in (6, 7, 8):    # summer
        return random.randint(25, 40)
    if month in (9, 10, 11):  # autumn
        return random.randint(18, 30)
    return random.randint(15, 30)

start_date = datetime(2025, 1, 1)
dates = [(start_date + timedelta(days=i)) for i in range(365)]
conditions = ["Sunny", "Cloudy", "Rainy", "Stormy", "Windy", "Foggy"]

rows = []
for dt in dates:
    month = dt.month
    temp = generate_seasonal_temp(month)
    # add some daily noise
    temp = int(temp + random.gauss(0, 2))
    # humidity tends to be higher when rainy/stormy/foggy
    cond = random.choices(
        conditions,
        weights=[40, 25, 20, 5, 6, 4],  # bias toward Sunny/Cloudy in generic locales
        k=1
    )[0]
    if cond == "Rainy":
        humidity = random.randint(70, 95)
    elif cond == "Stormy":
        humidity = random.randint(75, 98)
    elif cond == "Foggy":
        humidity = random.randint(80, 95)
    else:
        humidity = random.randint(35, 80)
    wind = random.randint(3, 30)
    rows.append({
        "Date": dt.strftime("%m-%d-%Y"),
        "Temperature": temp,
        "Condition": cond,
        "Humidity": humidity,
        "WindSpeed": wind
    })

df_year = pd.DataFrame(rows)
df_year.to_csv("weather_data.csv", index=False)
print("âœ… Created 'weather_data.csv' with", len(df_year), "rows (2025).")
df_year.head()



foggy = "/kaggle/input/multiclass-weather-dataset/dataset/foggy"
sunrise = "/kaggle/input/multiclass-weather-dataset/dataset/sunrise"
shine = "/kaggle/input/multiclass-weather-dataset/dataset/shine"
rainy = "/kaggle/input/multiclass-weather-dataset/dataset/rainy"
cloudy = "/kaggle/input/multiclass-weather-dataset/dataset/cloudy"
test = "/kaggle/input/multiclass-weather-dataset/dataset/alien_test"

print("Number of Images in Each Directory:")
print(f"Foggy: {len(os.listdir(foggy))}")
print(f"Sunrise: {len(os.listdir(sunrise))}")
print(f"Shine: {len(os.listdir(shine))}")
print(f"Rainy: {len(os.listdir(rainy))}")
print(f"Cloudy: {len(os.listdir(cloudy))}")

x = []
y = []
dataset =[]
def create_dataset(directory,dir_name):
    for i in tqdm.tqdm(os.listdir(directory)):
        full_path = os.path.join(directory,i)
        try:
            img = cv2.imread(full_path)
            img = cv2.resize(img,(150,150))
        except:
            continue
        x.append(img)
        y.append(dir_name)
    return x,y

x,y= create_dataset(foggy,"foggy")
x,y= create_dataset(sunrise,"sunrise")
x,y= create_dataset(shine,"shine")
x,y= create_dataset(rainy,"rainy")
x,y= create_dataset(cloudy,"cloudy")

x =  np.array(x)
y = np.array(y)
x.shape,y.shape

import seaborn as sns
plt.figure(figsize=(9,7))
plt.style.use("fivethirtyeight")
sns.countplot(y)
plt.show()

fig = plt.figure(figsize=(12,7))
for i in range(15):
    sample =  random.choice(range(len(x)))
    image = x[sample]
    category = y[sample]
    plt.subplot(3,5,i+1)
    plt.subplots_adjust(hspace=0.3)
    plt.imshow(image)
    plt.xlabel(category)

plt.tight_layout()
plt.show()

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y = le.fit_transform(y)

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)

img_size =150

x_train = np.array(x_train)/255.0
x_test = np.array(x_test)/255.0


x_train = x_train.reshape(-1,img_size,img_size,3)
y_train = np.array(y_train)

x_test = x_test.reshape(-1,img_size,img_size,3)
y_test = np.array(y_test)

from sklearn.preprocessing import LabelBinarizer
lb = LabelBinarizer()
y_train_lb = lb.fit_transform(y_train)
y_test_lb = lb.fit_transform(y_test)

y_train_lb.shape,y_test_lb.shape

"""><h3>Model building</h3>"""

from tensorflow.keras.applications.vgg19 import VGG19
vgg = VGG19(weights = "imagenet",include_top=False,input_shape=(img_size,img_size,3))

for layer in vgg.layers:
    layer.trainable = False

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Flatten,Dense
model =Sequential()
model.add(vgg)
model.add(Flatten())
model.add(Dense(5,activation="softmax"))

model.summary()

from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping

checkpoint = ModelCheckpoint("vgg19.h5",
                             monitor="val_accuracy",
                             verbose=1,
                             save_best_only=True,
                             save_weights_only=False)

earlystop = EarlyStopping(monitor="val_accuracy",
                          patience=5,
                          verbose=1)

unique,counts = np.unique(y_train_lb,return_counts=True)
print(unique,counts)

model.compile(optimizer="adam",
              loss="categorical_crossentropy",
              metrics=["accuracy"])
batch_size = 32
history = model.fit(x_train, y_train_lb,
                    epochs=15,
                    validation_data=(x_test, y_test_lb),
                    batch_size=batch_size,
                    verbose=1,
                    callbacks=[checkpoint, earlystop])

loss,accuracy = model.evaluate(x_test,y_test_lb)
print(f"Loss: {loss}")
print(f"Accuracy: {accuracy}")

model.summary()

# Get probabilities for each class
y_pred = model.predict(x_test)

# Convert probabilities to class indices
y_pred_classes = y_pred.argmax(axis=1)

# Show first 15 predictions
print(y_pred_classes[:15])

"""><h3>Classification Report</h3>"""

from sklearn.metrics import classification_report

# Convert predictions to class indices
y_pred_classes = y_pred.argmax(axis=1)

# If your y_test is one-hot encoded, convert it too:
if y_test.ndim > 1 and y_test.shape[1] > 1:
    y_test_classes = y_test.argmax(axis=1)
else:
    y_test_classes = y_test

# Now generate the report
print(classification_report(y_test_classes, y_pred_classes))

"""><h3>Confusion Matrix</h3>"""

from sklearn.metrics import confusion_matrix
from mlxtend.plotting import plot_confusion_matrix

# Convert predicted probabilities to class indices
y_pred_classes = y_pred.argmax(axis=1)

# Convert y_test to class indices if it's one-hot encoded
if y_test.ndim > 1 and y_test.shape[1] > 1:
    y_test_classes = y_test.argmax(axis=1)
else:
    y_test_classes = y_test

# Create confusion matrix
cm = confusion_matrix(y_test_classes, y_pred_classes)

# Plot confusion matrix
plot_confusion_matrix(
    conf_mat=cm,
    figsize=(8, 7),
    class_names=["cloudy", "foggy", "rainy", "shine", "sunrise"],
    show_normed=True
);

"""><h3>Learning Curve:</h3>"""

import matplotlib.pyplot as plt

plt.style.use("ggplot")
fig = plt.figure(figsize=(12,6))
epochs = range(1, len(history.history["accuracy"]) + 1)  # safer than hardcoding 16

# Accuracy plot
plt.subplot(1,2,1)
plt.plot(epochs, history.history["accuracy"], "go-")
plt.plot(epochs, history.history["val_accuracy"], "ro-")
plt.title("Model Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend(["Train","Val"], loc="upper left")

# Loss plot
plt.subplot(1,2,2)
plt.plot(epochs, history.history["loss"], "go-")
plt.plot(epochs, history.history["val_loss"], "ro-")
plt.title("Model Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend(["Train","Val"], loc="upper left")

plt.show()

"""><h3>Model Performance:</h3>"""

plt.figure(figsize=(12,9))
plt.style.use("ggplot")
for i in range(10):
    sample = random.choice(range(len(x_test)))
    plt.subplot(2,5,i+1)
    plt.subplots_adjust(hspace=0.3)
    plt.imshow(x_test[sample])
    plt.xlabel(f"Actual: {y_test[sample]}\n Predicted: {y_pred[sample]}")

plt.tight_layout()
plt.show()
